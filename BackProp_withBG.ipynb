{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BackProp_withBG.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyMqM2KaPQIDZqx2A1lU10Fy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hirokame/BPBG/blob/main/BackProp_withBG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import cupy as cp\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "transform = transforms.ToTensor()\n",
        "        \n",
        "\n",
        "# download MNIST dataset\n",
        "train_data = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST('../data', train=False, transform=transform)\n",
        "\n",
        "# set DataLoader\n",
        "batch_size = 4\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "ES50tAgNsevk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Set the layer number and size'''\n",
        "input_size = 784\n",
        "hidden_size = [100,50]\n",
        "output_size = 10\n",
        "\n",
        "\n",
        "'''Set the device'''\n",
        "cuda0 = torch.device('cuda:0')\n",
        "\n",
        "\n",
        "'''Initialize the weight'''\n",
        "w12 = (torch.rand(input_size, hidden_size[0])*2-1).cuda()\n",
        "w23 = (torch.rand(hidden_size[0], hidden_size[1])*2-1).cuda()\n",
        "w34 = (torch.rand(hidden_size[1], output_size)*2-1).cuda()\n",
        "\n",
        "\n",
        "\n",
        "'''Lateral inhibition inside the layer'''\n",
        "lateral_inhibition = False\n",
        "\n",
        "w11 = -torch.div(torch.rand(input_size, input_size), input_size).cuda()\n",
        "for i in range(input_size):\n",
        "  w11[i,i]=0\n",
        "w22 = -torch.div(torch.rand(hidden_size[0], hidden_size[0]), hidden_size[0]).cuda()\n",
        "for i in range(hidden_size[0]):\n",
        "  w22[i,i]=0\n",
        "w33 = -torch.div(torch.rand(hidden_size[1], hidden_size[1]), hidden_size[1]).cuda()\n",
        "for i in range(hidden_size[1]):\n",
        "  w33[i,i]=0\n",
        "\n",
        "\n",
        "\n",
        "'''Call Torch.nn functions'''\n",
        "identity = nn.Identity()\n",
        "# act = nn.Sigmoid()\n",
        "act = nn.ReLU()\n",
        "softmax = nn.Softmax()\n",
        "\n",
        "\n",
        "''' Hyper parameters '''\n",
        "\n",
        "eta1 = 3.0e-2  # learning rate\n",
        "eta2 = 3.0e-2\n",
        "eta3 = 3.0e-2\n",
        "\n",
        "alpha1 = 0  # Leaky dynamics of weights\n",
        "alpha2 = 0\n",
        "\n",
        "beta = 1.0 # synapse weight modification of BG loop\n",
        "\n",
        "epoch = 200  # training epochs\n",
        "\n",
        "eps = 1.0e-7\n",
        "\n",
        "'''Training and Test log'''\n",
        "history = {\n",
        "    'train_loss':[],\n",
        "    'test_loss':[],\n",
        "    'test_acc':[],\n",
        "    'pred_hist': np.empty((0,10))\n",
        "}\n",
        "\n",
        "latest_loss = 2.0\n",
        "loss_list = []\n",
        "for e in range(epoch):\n",
        "  print('epoch {} start'.format(e+1))\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "\n",
        "    '''set the device '''\n",
        "    data  = (data).cuda()\n",
        "    target = target.cuda()\n",
        "\n",
        "    ''' Training Part '''\n",
        "\n",
        "    '''forward propagation'''\n",
        "    x1_in = torch.flatten(data,1) # (batch,28,28)->(batch,784)\n",
        "    if lateral_inhibition:\n",
        "      x1_in = act(x1_in + torch.matmul(x1_in, w11))\n",
        "    x1_out = identity(x1_in)\n",
        "\n",
        "\n",
        "    x2_in = act(torch.matmul(x1_out, w12)) # (batch,784)->(batch,100), relu\n",
        "    if lateral_inhibition:\n",
        "      x2_in = act(x2_in + torch.matmul(x2_in, w22))\n",
        "    x2_in = torch.div(x2_in, torch.mean(x2_in)+eps) # normalization\n",
        "    x2_out = identity(x2_in)\n",
        "\n",
        "\n",
        "    x3_in = act(torch.matmul(x2_out, w23)) # (batch,100)->(batch,50), relu\n",
        "    if lateral_inhibition:\n",
        "      x3_in = act(x3_in + torch.matmul(x3_in, w33))\n",
        "    x3_in = torch.div(x3_in, torch.mean(x3_in)+eps) # normalization\n",
        "    x3_out = identity(x3_in)\n",
        "    \n",
        "\n",
        "    x4_in = act(torch.matmul(x3_out, w34)) # (batch,50)->(batch,10), relu\n",
        "    x4_in = torch.div(x4_in, torch.mean(x4_in)+eps) # normalization\n",
        "    x4_out = identity(x4_in)\n",
        "    \n",
        "\n",
        "    output = softmax(x4_out)\n",
        "    \n",
        "\n",
        "    ''' label encoding -> one-hot encoding '''\n",
        "    target_oh = F.one_hot(target, num_classes = 10)\n",
        "\n",
        "\n",
        "    '''loss: subtraction between output and target'''\n",
        "    loss = target_oh - output\n",
        "    \n",
        "\n",
        "    '''\n",
        "    sim34(i,j): similarity matrix between layer3 node(i) and layer4 node(j)\n",
        "    sim24(i,j): similarity matrix between layer2 node(i) and layer4 node(j)\n",
        "    similarity = absolute value of subtraction between two nodes\n",
        "    '''\n",
        "    grid3 = torch.tile(x3_out, (1,output_size)).reshape(batch_size, output_size, hidden_size[1]).transpose(1,2) # (32,50,10)\n",
        "    grid4 = torch.tile(x4_out, (1,hidden_size[1])).reshape(batch_size, hidden_size[1], output_size) # (32,50,10)\n",
        "    sim34 = torch.reciprocal(torch.abs(grid4-grid3)+1.0e-7) # similarity matrix = (32,50,10)\n",
        "  \n",
        "    grid2 = torch.tile(x2_out, (1,output_size)).reshape(batch_size, output_size, hidden_size[0]).transpose(1,2) #(32,100,10)\n",
        "    grid4 = torch.tile(x4_out, (1,hidden_size[0])).reshape(batch_size, hidden_size[0], output_size) # (32,100,10)\n",
        "    sim24 = torch.reciprocal(torch.abs(grid4-grid2)+1.0e-7) # similarity matrix = (32,100,10)\n",
        "\n",
        "\n",
        "    # Calculate the gradient like value (delta * similarity * trace)\n",
        "    x4_in = beta*loss # (32,10)\n",
        "    x3_in = beta*torch.einsum('bn,bnm->bm',loss,sim34.transpose(1,2)) # (32,50) + (32,10)@(32,10,50) = (32,50)\n",
        "    x2_in = beta*torch.einsum('bn,bnm->bm',loss,sim24.transpose(1,2)) # (32,100) + (32,10)@(32,10,100) = (32,100)  \n",
        "\n",
        "    # x4_in = act(x4_in + beta*loss) # (32,10)\n",
        "    # x3_in = act(x3_in + beta*torch.einsum('bn,bnm->bm',loss,sim34.transpose(1,2))) # (32,50) + (32,10)@(32,10,50) = (32,50)\n",
        "    # x2_in = act(x2_in + beta*torch.einsum('bn,bnm->bm',loss,sim24.transpose(1,2))) # (32,100) + (32,10)@(32,10,100) = (32,100)  \n",
        "    \n",
        "\n",
        "    # Dopamine modulation (if dopamine released, increase lr)\n",
        "    current_loss = torch.sum(torch.abs(loss))/batch_size\n",
        "    eta_dop1 = max(eta1, eta1*(current_loss/latest_loss)**4)\n",
        "    eta_dop2 = max(eta2, eta2*(current_loss/latest_loss)**4)\n",
        "    eta_dop3 = max(eta3, eta3*(current_loss/latest_loss)**4)\n",
        "\n",
        "    # Hebbinan Plasticity (with leaky dynamics)\n",
        "    w12 +=  -alpha1*w12 + eta_dop1*torch.mean(torch.einsum('bn,bm->bnm',x1_out, x2_in), 0) # hebbian plasticity -> take an average through the batch -> multiply the learning rate\n",
        "    w23 +=  -alpha2*w23 + eta_dop2*torch.mean(torch.einsum('bn,bm->bnm',x2_out, x3_in), 0)\n",
        "    w34 +=  eta_dop3*torch.mean(torch.einsum('bn,bm->bnm',x3_out, x4_in), 0)\n",
        "    # print(\"w12\", w12.shape, w12)\n",
        "    # print(\"w23\", w23.shape, w23)\n",
        "    # print(\"w34\", w34.shape, w34)\n",
        "\n",
        "\n",
        "    # Learning rate update (simulated annealing)\n",
        "    if batch_idx%2 == 0:\n",
        "      loss_list.append(current_loss.cpu())\n",
        "      if batch_idx%100 == 0:\n",
        "        loss_ave = np.mean(loss_list)\n",
        "        if loss_ave < 0.5*latest_loss:\n",
        "          latest_loss = loss_ave\n",
        "          eta1 *= 0.8\n",
        "          eta2 *= 0.8\n",
        "          eta3 *= 0.8\n",
        "        if batch_idx%1000 == 0:\n",
        "          print(loss_ave)\n",
        "        # if batch_idx % 3000 == 0:\n",
        "        #   print('Epoch{}, {}/60000, Train loss:{}'.format((e+1), batch_size*(batch_idx+1), torch.sum(torch.abs(loss))/batch_size))\n",
        "      loss_list = []\n",
        "\n",
        "  train_loss = torch.sum(torch.abs(loss))/batch_size\n",
        "  \n",
        "  # print('Train loss: {}'.format(train_loss))\n",
        "  history['train_loss'].append(train_loss.cpu())\n",
        "\n",
        "  ''' Test Part '''\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  pred_hist = np.empty(0)\n",
        "  for data, target in test_loader:\n",
        "    data = data.cuda()\n",
        "    target = target.cuda()\n",
        "\n",
        "    # forward propagation\n",
        "    x1_in = torch.flatten(data,1) # (batch,28,28)->(batch,784)\n",
        "    if lateral_inhibition:\n",
        "      x1_in = act(x1_in + torch.matmul(x1_in, w11))\n",
        "    x1_out = identity(x1_in)\n",
        "\n",
        "\n",
        "    x2_in = act(torch.matmul(x1_out, w12)) # (batch,784)->(batch,100), relu\n",
        "    if lateral_inhibition:\n",
        "      x2_in = act(x2_in + torch.matmul(x2_in, w22))\n",
        "    x2_in = torch.div(x2_in, torch.mean(x2_in)+eps) # normalization\n",
        "    x2_out = identity(x2_in)\n",
        "\n",
        "\n",
        "    x3_in = act(torch.matmul(x2_out, w23)) # (batch,100)->(batch,50), relu\n",
        "    if lateral_inhibition:\n",
        "      x3_in = act(x3_in + torch.matmul(x3_in, w33))\n",
        "    x3_in = torch.div(x3_in, torch.mean(x3_in)+eps) # normalization\n",
        "    x3_out = identity(x3_in)\n",
        "\n",
        "\n",
        "    x4_in = act(torch.matmul(x3_out, w34)) # (batch,50)->(batch,10), relu\n",
        "    x4_in = torch.div(x4_in, torch.mean(x4_in)+eps) # normalization\n",
        "    x4_out = identity(x4_in)\n",
        "\n",
        "\n",
        "    output = softmax(x4_out)\n",
        "    target_oh = F.one_hot(target, num_classes = 10)\n",
        "    loss = target_oh - output\n",
        "\n",
        "\n",
        "    test_loss += torch.sum(torch.abs(loss))\n",
        "    pred = output.argmax(dim=1, keepdim=True)\n",
        "    correct += pred.eq(target.view_as(pred)).sum()\n",
        "    pred_hist = np.append(pred_hist, pred.cpu())\n",
        "  \n",
        "  # print('pred', pred.reshape(1,-1))\n",
        "  # print('target', target)\n",
        "  test_loss /= 10000\n",
        "  acc = correct/10000\n",
        "  histo = np.bincount(pred_hist.astype('int64').ravel(), minlength=10)\n",
        "  print('Test loss: {}, Test acc: {}'.format(test_loss, acc))\n",
        "\n",
        "  history['test_loss'].append(test_loss.cpu())\n",
        "  history['test_acc'].append(correct.cpu()/10000)\n",
        "  history['pred_hist'] = np.vstack((history['pred_hist'],histo))\n",
        "  print('histogram', histo)\n",
        "  print('latest_loss', latest_loss,'eta', eta1,eta2,eta3)\n",
        "\n",
        "  # print('w12',w12[0][:10])\n",
        "  # print('w23',w23[0][:10])\n",
        "  # print('w34',w34[0])\n",
        "\n",
        "# plot figures\n",
        "plt.figure()\n",
        "plt.plot(range(1, epoch+1), history['train_loss'], label='train_loss')\n",
        "plt.plot(range(1, epoch+1), history['test_loss'], label='test_loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(range(1, epoch+1), history['test_acc'])\n",
        "plt.title('test accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r-xPjy4Vm6k8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.bincount([0,2,3,4,5,6,7,8])"
      ],
      "metadata": {
        "id": "s0u0wdNpMhxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (10,4))\n",
        "plt.title('Train/Test loss')\n",
        "plt.plot(range(1, 201), history['train_loss'], label='train_loss')\n",
        "plt.plot(range(1, 201), history['test_loss'], label='test_loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure(figsize = (10,4))\n",
        "plt.plot(range(1, 201), history['test_acc'], label='test_acc')\n",
        "plt.title('test accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend()\n",
        "\n",
        "pred_hist = history['pred_hist'].reshape(10,200)\n",
        "plt.figure(figsize = (10,4))\n",
        "for i in range(2):\n",
        "  plt.plot(range(1, 201), pred_hist[i], label='digit {}'.format(i))\n",
        "plt.title('digit histogram')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "taUfROJGf6mZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('w12',w12.mean(dim=0))\n",
        "print('w23',w23.mean(dim=0))\n",
        "print('w34',w34.mean(dim=0))"
      ],
      "metadata": {
        "id": "ds9vt1-DY2KB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyNet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyNet, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(784, 100)\n",
        "        self.fc2 = torch.nn.Linear(100, 50)\n",
        "        self.fc3 = torch.nn.Linear(50, 10)\n",
        " \n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = torch.sigmoid(x)\n",
        "        x = self.fc2(x)\n",
        "        x = torch.sigmoid(x)\n",
        "        x = self.fc3(x)\n",
        " \n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "W9AqGnORyvJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  # set the training epoch\n",
        "  epoch = 5\n",
        "  \n",
        "  # save the log\n",
        "  history = {\n",
        "      'train_loss':[],\n",
        "      'test_loss':[],\n",
        "      'test_acc':[]\n",
        "  }\n",
        "\n",
        "  # initiate the network\n",
        "  net: torch.nn.Module = MyNet()\n",
        "  \n",
        "  # set the optimizer\n",
        "  optimizer = torch.optim.Adam(params=net.parameters(), lr=1.0e-3)\n",
        "\n",
        "  for e in range(epoch):\n",
        "\n",
        "    #####     Training Part     #####\n",
        "\n",
        "    loss = None\n",
        "\n",
        "    net.train(True)\n",
        "\n",
        "    for i, (data, target) in enumerate(train_loader):\n",
        "\n",
        "      data = data.view(-1, 784)\n",
        "\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      output = net(data)\n",
        "      loss = F.nll_loss(output, target)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if i%100 == 0:\n",
        "        print(\"Training log: {} epoch ({}/60000 train data) Loss: {})\".format(e+1, (i+1)*64, loss.item()))\n",
        "      \n",
        "    history['train_loss'].append(loss.detach().numpy())\n",
        "\n",
        "\n",
        "    #####     Test Part     #####\n",
        "\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for data, target in test_loader:\n",
        "        data = data.view(-1,784)\n",
        "        output = net(data)\n",
        "        test_loss += F.nll_loss(output, target, reduction=\"sum\").item()\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= 10000\n",
        "\n",
        "    print('Test loss (avg): {}, Accuracy: {}'.format(test_loss, correct/10000))\n",
        "\n",
        "    history['test_loss'].append(test_loss)\n",
        "    history['test_acc'].append(correct/10000)\n",
        "\n",
        "  # plot figure\n",
        "  plt.figure()\n",
        "  plt.plot(range(1, epoch+1), history['train_loss'], label='train_loss')\n",
        "  plt.plot(range(1, epoch+1), history['test_loss'], label='test_loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend()\n",
        "  plt.savefig('loss.png')\n",
        "\n",
        "  plt.figure()\n",
        "  plt.plot(range(1, epoch+1), history['test_acc'])\n",
        "  plt.title('test accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.savefig('test_acc.png')"
      ],
      "metadata": {
        "id": "pspQijij3HTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eGi0XTcV7rxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "75T1vnq78Tlf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}