{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BackProp_withBG.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyMrjEIzmK8sL2CwzJOJbpmP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hirokame/BPBG/blob/main/BackProp_withBG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import cupy as cp\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "\n",
        "# download MNIST dataset\n",
        "train_data = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST('../data', train=False, transform=transform)\n",
        "\n",
        "# set DataLoader\n",
        "batch_size = 16\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "ES50tAgNsevk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the layer number and size\n",
        "input_size = 784\n",
        "hidden_size = [100,50]\n",
        "output_size = 10\n",
        "\n",
        "\n",
        "# Set the device\n",
        "cuda0 = torch.device('cuda:0')\n",
        "\n",
        "\n",
        "# Initialize the weight\n",
        "w12 = torch.rand(input_size, hidden_size[0]).cuda()\n",
        "w23 = torch.rand(hidden_size[0], hidden_size[1]).cuda()\n",
        "w34 = torch.rand(hidden_size[1], output_size).cuda()\n",
        "\n",
        "\n",
        "# Lateral inhibition inside the layer\n",
        "lateral_inhibition = False\n",
        "\n",
        "w11 = -torch.div(torch.rand(input_size, input_size), input_size).cuda()\n",
        "for i in range(input_size):\n",
        "  w11[i,i]=0\n",
        "w22 = -torch.div(torch.rand(hidden_size[0], hidden_size[0]), hidden_size[0]).cuda()\n",
        "for i in range(hidden_size[0]):\n",
        "  w22[i,i]=0\n",
        "w33 = -torch.div(torch.rand(hidden_size[1], hidden_size[1]), hidden_size[1]).cuda()\n",
        "for i in range(hidden_size[1]):\n",
        "  w33[i,i]=0\n",
        "\n",
        "\n",
        "# Call Torch.nn functions\n",
        "identity = nn.Identity()\n",
        "relu = nn.ReLU()\n",
        "softmax = nn.Softmax()\n",
        "\n",
        "# Hyper parameter\n",
        "eta = 1.0e-2\n",
        "alpha = 5.0e-3\n",
        "beta = 1.0e-2\n",
        "epoch = 1\n",
        "\n",
        "# Training\n",
        "Loss = {}\n",
        "for e in range(epoch):\n",
        "  print('epoch {} start'.format(e+1))\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "\n",
        "    # set the device \n",
        "    data  = data.cuda()\n",
        "    target = target.cuda()\n",
        "\n",
        "\n",
        "    # forward propagation\n",
        "    x1_in = torch.flatten(data,1) # (batch,28,28)->(batch,784)\n",
        "    if lateral_inhibition:\n",
        "      x1_in = relu(x1_in + torch.matmul(x1_in, w11))\n",
        "    x1_out = identity(x1_in)\n",
        "\n",
        "    x2_in = relu(torch.matmul(x1_out, w12)) # (batch,784)->(batch,100), relu\n",
        "    if lateral_inhibition:\n",
        "      x2_in = relu(x2_in + torch.matmul(x2_in, w22))\n",
        "    x2_in = torch.div(x2_in, torch.mean(x2_in)) # normalization\n",
        "    x2_out = identity(x2_in)\n",
        "    # print('x2_in', x2_in)\n",
        "\n",
        "    x3_in = relu(torch.matmul(x2_out, w23)) # (batch,100)->(batch,50), relu\n",
        "    if lateral_inhibition:\n",
        "      x3_in = relu(x3_in + torch.matmul(x3_in, w33))\n",
        "    x3_in = torch.div(x3_in, torch.mean(x3_in)) # normalization\n",
        "    x3_out = identity(x3_in)\n",
        "    # print('x3_in', x3_in)\n",
        "\n",
        "    x4_in = relu(torch.matmul(x3_out, w34)) # (batch,50)->(batch,10), relu\n",
        "    x4_in = torch.div(x4_in, torch.mean(x4_in)) # normalization\n",
        "    x4_out = identity(x4_in)\n",
        "    # print('x4_in', x4_in)\n",
        "\n",
        "    output = softmax(x4_out)\n",
        "\n",
        "    target = F.one_hot(target, num_classes = 10) # label encoding -> one-hot encoding\n",
        "\n",
        "\n",
        "    # loss: subtraction between output and target\n",
        "    loss = target - output\n",
        "\n",
        "\n",
        "    # sim34(i,j): similarity matrix between layer3 node(i) and layer4 node(j)\n",
        "    # sim24(i,j): similarity matrix between layer2 node(i) and layer4 node(j)\n",
        "    # similarity = absolute value of subtraction between two nodes\n",
        "\n",
        "    grid3 = torch.tile(x3_out, (1,output_size)).reshape(batch_size, output_size, hidden_size[1]).transpose(1,2) # (32,50,10)\n",
        "    grid4 = torch.tile(x4_out, (1,hidden_size[1])).reshape(batch_size, hidden_size[1], output_size) # (32,50,10)\n",
        "    sim34 = torch.abs(grid4-grid3) # similarity matrix = (32,50,10)\n",
        "\n",
        "    grid2 = torch.tile(x2_out, (1,output_size)).reshape(batch_size, output_size, hidden_size[0]).transpose(1,2) #(32,100,10)\n",
        "    grid4 = torch.tile(x4_out, (1,hidden_size[0])).reshape(batch_size, hidden_size[0], output_size) # (32,100,10)\n",
        "    sim24 = torch.abs(grid4-grid2) # similarity matrix = (32,100,10)\n",
        "\n",
        "\n",
        "    # Calculate the gradient like value (delta * similarity * trace)\n",
        "    x4_in = relu(x4_in + beta*loss) # (32,10)\n",
        "    x3_in = relu(x3_in + beta*torch.einsum('bn,bnm->bm',loss,sim34.transpose(1,2))) # (32,50) + (32,10)@(32,10,50) = (32,50)\n",
        "    x2_in = relu(x2_in + beta*torch.einsum('bn,bnm->bm',loss,sim24.transpose(1,2))) # (32,100) + (32,10)@(32,10,100) = (32,100)    \n",
        "    \n",
        "    # Hebbinan Plasticity\n",
        "    w12 += -alpha*w12 + eta*torch.mean(torch.einsum('bn,bm->bnm',x1_out, x2_in), 0) # hebbian plasticity -> take an average through the batch -> multiply the learning rate\n",
        "    w23 += -alpha*w23 + eta*torch.mean(torch.einsum('bn,bm->bnm',x2_out, x3_in), 0)\n",
        "    w34 += -alpha*w34 + eta*torch.mean(torch.einsum('bn,bm->bnm',x3_out, x4_in), 0)\n",
        "\n",
        "  Loss['eta{}_aplha{}_beta{}'.format(eta,alpha,beta)] = torch.sum(torch.abs(loss)).to('cpu').detach().numpy()/batch_size\n",
        "  # print('w12_sample',w12[0][:10])\n",
        "  # print('w23_sample',w23[0][:10])\n",
        "  # print('w34_sample',w34[0])"
      ],
      "metadata": {
        "id": "r-xPjy4Vm6k8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Loss"
      ],
      "metadata": {
        "id": "EMnJV2CHh_80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.arange(len(Loss))\n",
        "plt.figure(figsize=(30,8))\n",
        "plt.plot(x,Loss)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9QmUa2Niv-7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eRjSGjdrX2_g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}